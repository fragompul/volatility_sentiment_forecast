{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb2403c-9147-4a06-9c29-92abe47be277",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy python-dotenv pandas --quiet\n",
    "\n",
    "import os\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a26c51c0-e1ce-449d-865d-c98265cd49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "# Twitter API credentials\n",
    "BEARER_TOKEN = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "# Authenticate with Twitter API v2\n",
    "client = tweepy.Client(bearer_token=BEARER_TOKEN, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6e9f0d-808c-470e-9a02-3c1951d12df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tickers and corresponding names\n",
    "ticker_name_map = {\n",
    "    \"AAPL\": \"Apple\",\n",
    "    \"TSLA\": \"Tesla\",\n",
    "    \"MSFT\": \"Microsoft\",\n",
    "    \"GOOGL\": \"Google\",\n",
    "    \"AMZN\": \"Amazon\",\n",
    "    \"SP500\": \"S&P 500\",\n",
    "    \"NASDAQ\": \"NASDAQ\",\n",
    "    \"IBEX35\": \"IBEX 35\"\n",
    "}\n",
    "\n",
    "# Companies/keywords we want to track\n",
    "keywords1 = [\"AAPL\", \"TSLA\", \"MSFT\", \"GOOGL\", \"AMZN\"]\n",
    "keywords2 = [\"SP500\", \"NASDAQ\", \"IBEX35\"]\n",
    "max_tweets_per_keyword = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd88fa6-6ab9-4c5b-985e-56dd4bc0297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tweets(keywords):\n",
    "    for ticker in keywords:\n",
    "        company_name = ticker_name_map.get(ticker, \"\")\n",
    "        print(f\"Searching tweets for {ticker} ({company_name})...\")\n",
    "\n",
    "        query = (\n",
    "            f'({ticker} OR ${ticker} OR \"#{ticker}\" OR \"{company_name}\"'\n",
    "            f') (stock OR price OR shares OR market OR buy OR sell OR volatility OR bullish OR bearish OR earnings) '\n",
    "            f'lang:en -is:retweet'\n",
    "        )\n",
    "        print(f\"Query: {query}\")\n",
    "\n",
    "        tweets_per_keyword = []\n",
    "\n",
    "        tweets = tweepy.Paginator(\n",
    "            client.search_recent_tweets,\n",
    "            query=query,\n",
    "            tweet_fields=[\"created_at\", \"public_metrics\", \"text\", \"lang\"],\n",
    "            max_results=100\n",
    "        ).flatten(limit=max_tweets_per_keyword)\n",
    "\n",
    "        count = 0\n",
    "        for tweet in tweets:\n",
    "            tweets_per_keyword.append({\n",
    "                \"ticker\": ticker,\n",
    "                \"date\": tweet.created_at,\n",
    "                \"text\": tweet.text,\n",
    "                \"likes\": tweet.public_metrics[\"like_count\"],\n",
    "                \"retweets\": tweet.public_metrics[\"retweet_count\"]\n",
    "            })\n",
    "            count += 1\n",
    "\n",
    "        print(f\"Collected {count} tweets for {ticker}\")\n",
    "\n",
    "        df_partial = pd.DataFrame(tweets_per_keyword)\n",
    "        df_partial.to_csv(f\"../data/raw/tweets_partial_{ticker}.csv\", index=False)\n",
    "        print(f\"Saved partial dataset: tweets_partial_{ticker}.csv\")\n",
    "\n",
    "    print(\"Finished collecting tweets for current batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d51dd0-baf6-4bd2-abab-5629e03f2547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 689 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching tweets for MSFT (Microsoft)...\n",
      "Query: (MSFT OR $MSFT OR \"#MSFT\" OR \"Microsoft\" ) (stock OR price OR shares OR market OR buy OR sell OR volatility OR bullish OR bearish OR earnings) lang:en -is:retweet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 902 seconds.\n"
     ]
    }
   ],
   "source": [
    "collect_tweets(keywords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa5c6f-4f03-4280-8d35-4c310a2c6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_tweets(keywords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75974553-fecf-4adb-be1c-5a5ed8cbdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all dataframes\n",
    "all_keywords = keywords1 + keywords2\n",
    "all_dfs = []\n",
    "\n",
    "for ticker in all_keywords:\n",
    "    csv_path = f\"../data/raw/tweets_partial_{ticker}.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        all_dfs.append(df)\n",
    "    else:\n",
    "        print(f\"Warning: File {csv_path} not found, skipping.\")\n",
    "\n",
    "if all_dfs:\n",
    "    df_combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    final_path = \"../data/raw/tweet_finance.csv\"\n",
    "    df_combined.to_csv(final_path, index=False)\n",
    "    print(f\"Saved combined dataset {final_path} with {len(df_combined)} tweets total.\")\n",
    "    print(df_combined.head())\n",
    "else:\n",
    "    print(\"No tweet data found to combine.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
